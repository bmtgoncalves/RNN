{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><h1>LSTM Tutorial in Keras</h1>\n",
    "<div>Bruno Gon√ßalves</div>\n",
    " <a href=\"http://www.bgoncalves.com/\" ><span style=\"font-size:90%;\">www.bgoncalves.com</span></a>\n",
    " </div>\n",
    "<!--h2 align=\"center\", style=\"font-size=150%\">Part I</h2-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Embedding\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7845\n",
      "['the', 'and', 'I', 'of', 'to', 'my', 'a', 'in', 'was', 'that', 'me', 'with', 'had', 'which', 'but', 'you', 'not', 'his', 'for', 'as']\n"
     ]
    }
   ],
   "source": [
    "filename = \"Frankenstein.txt\"\n",
    "\n",
    "word_regex = re.compile(r'\\w+', re.U)\n",
    "\n",
    "text = \"\".join(open(filename, 'rt').readlines())\n",
    "\n",
    "text_words = word_regex.findall(text)\n",
    "\n",
    "word_counts = Counter(text_words)\n",
    "\n",
    "word_count_items = list(word_counts.items())\n",
    "word_count_items.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "word_list = [word for word, count in word_count_items]\n",
    "word_dict = dict(zip(word_list, range(len(word_count_items))))\n",
    "\n",
    "number_words = len(word_counts)\n",
    "\n",
    "print(number_words)\n",
    "print(word_list[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict['I']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create inputs and labels\n",
    "\n",
    "We use a window of size __sequence_length__ size that moves __step__ by step over the text. Each such sequence corresponds to a sentence and a row in __input_words__. The word at the next position is our expected __label_words__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78596\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 10\n",
    "step = 1\n",
    "\n",
    "input_words = []\n",
    "label_words = []\n",
    "for i in range(0, len(text_words) - sequence_length, step):\n",
    "    input_words.append(text_words[i:i + sequence_length])\n",
    "    label_words.append(text_words[i + sequence_length])\n",
    "    \n",
    "print(len(input_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['to', 'his', 'cabin', 'Even', 'broken', 'in', 'spirit', 'as', 'he', 'is']\n",
      "no\n",
      "['his', 'cabin', 'Even', 'broken', 'in', 'spirit', 'as', 'he', 'is', 'no']\n"
     ]
    }
   ],
   "source": [
    "print(input_words[5000])\n",
    "print(label_words[5000])\n",
    "print(input_words[5001])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we one hot encode our words, generating a 3D tensor **X** of dimensions *len(input_words)* $\\times$ *sequence_length* $\\times$ *number_words*. This tensor will be all zeros except for one position per vector which will be one. For the sake of memory efficiency, we define it as a boolean tensor.\n",
    "\n",
    "Conversely, our expected output **y** is now a 2D matrix of dimensions *len(input_words)* $\\times$ *number_words*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78596, 10, 7845)\n"
     ]
    }
   ],
   "source": [
    "X = np.zeros((len(input_words), sequence_length, number_words), dtype=np.bool)\n",
    "y = np.zeros((len(input_words), number_words), dtype=np.bool)\n",
    "\n",
    "for i, sentence in enumerate(input_words):\n",
    "    for t, word in enumerate(sentence):\n",
    "        X[i, t, word_dict[word]] = 1\n",
    "\n",
    "    y[i, word_dict[label_words[i]]] = 1\n",
    "    \n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some useful constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "NUM_ITERATIONS = 1\n",
    "NUM_EPOCHS_PER_ITERATION = 3\n",
    "NUM_PREDS_PER_EPOCH = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_simple = Sequential()\n",
    "model_simple.add(GRU(HIDDEN_SIZE, input_shape=(sequence_length, number_words)))\n",
    "model_simple.add(Dense(number_words))\n",
    "model_simple.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Compilation\n",
    "\n",
    "The final remaining step in defining our model is to \"compile\" it. This signals to Keras that we are done defining layers as well as defining the loss function and optimizer we wish to use. After this step we are ready to start learning our model!\n",
    "\n",
    "Here we opt to use the **RMSprop** optimizer with a learning rate of **0.01**, together with a **categorical_crossentropy** loss function. Naturally, other choices are possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=0.01)\n",
    "model_simple.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, let's take a look at the models internal representation. Keras provides us with a simple way of doing this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_1 (GRU)                  (None, 128)               3062016   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7845)              1012005   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 7845)              0         \n",
      "=================================================================\n",
      "Total params: 4,074,021\n",
      "Trainable params: 4,074,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "pprint(model_simple.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we three layers, **GRU**, **Dense** and **Activation**. The values of all their parameters (mostly default) are also clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Traininig\n",
    "\n",
    "To train the model we need just call the model.fit method with the appropriate parameters. Here we define a training function that combines training multiple iterations with generating predictions based on a specific sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, val_split=0.1):\n",
    "    for iteration in range(NUM_ITERATIONS):\n",
    "        print(\"=\" * 50)\n",
    "        print(\"Iteration #: %d\" % (iteration))\n",
    "        \n",
    "        # We keep the history result of the trainig step. This provides us with \n",
    "        # valueable information about the training procedure that we may use later.\n",
    "        history = model.fit(X_train, y_train, \n",
    "                            validation_split=val_split,\n",
    "                            batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)\n",
    "\n",
    "        # Choose a specific sentence to use to generate the predictions \n",
    "        # and generate NUM_PREDS_PER_EPOCH predictions.\n",
    "        test_idx = 1502 \n",
    "        test_words = input_words[test_idx]\n",
    "        print(\"Generating from seed: \\\"%s\\\"\" % (\" \".join(test_words)))\n",
    "\n",
    "        for i in range(NUM_PREDS_PER_EPOCH):\n",
    "            Xtest = np.zeros((1, sequence_length, number_words))\n",
    "\n",
    "            for i, word in enumerate(test_words):\n",
    "                Xtest[0, i, word_dict[word]] = 1\n",
    "\n",
    "            pred = model.predict(Xtest, verbose=0)[0]\n",
    "            ypred = word_list[np.argmax(pred)]\n",
    "            print(\" \".join(test_words), \"=>\", ypred, end=\" \")\n",
    "\n",
    "            # Add the new word to the test instance and \n",
    "            # move the window forward 1 step\n",
    "            test_words = test_words[1:] + [ypred]\n",
    "\n",
    "            print()\n",
    "            \n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model!\n",
    "\n",
    "Unfortunately, this takes a looooooong time without a GPU, so let's use just a small subset of our data for a quick check that all is fine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Iteration #: 0\n",
      "Train on 1500 samples, validate on 500 samples\n",
      "Epoch 1/3\n",
      "1500/1500 [==============================] - 65s 44ms/step - loss: 8.0080 - val_loss: 7.0498\n",
      "Epoch 2/3\n",
      "1500/1500 [==============================] - 52s 34ms/step - loss: 6.2252 - val_loss: 7.2898\n",
      "Epoch 3/3\n",
      "1500/1500 [==============================] - 47s 31ms/step - loss: 5.8157 - val_loss: 7.2729\n",
      "Generating from seed: \"of success there will be none to participate my joy\"\n",
      "of success there will be none to participate my joy => the \n",
      "success there will be none to participate my joy the => the \n",
      "there will be none to participate my joy the the => the \n",
      "will be none to participate my joy the the the => the \n",
      "be none to participate my joy the the the the => the \n",
      "none to participate my joy the the the the the => the \n",
      "to participate my joy the the the the the the => the \n",
      "participate my joy the the the the the the the => of \n",
      "my joy the the the the the the the of => the \n",
      "joy the the the the the the the of the => of \n"
     ]
    }
   ],
   "source": [
    "history, model_simple = train_model(model_simple, X[:2000, :,:], y[:2000, :], 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model we trained for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides us with a simple way to save the model condiguration. Unfortunately, this does not include the training history, just the final result. We save history in a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_filename = \"lstm_bad\"\n",
    "history_dict = history.history\n",
    "\n",
    "model_simple.save(model_filename + \".h5\")\n",
    "json.dump(history_dict, open(model_filename + \".json\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load a well trained model that is available to us! We also load the training history so that we may visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history_dict = json.load(open(\"lstm_simple.json\", \"r\"))\n",
    "model_simple = load_model(\"lstm_simple.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the model architecture matches what we expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'class_name': 'LSTM',\n",
      "  'config': {'activation': 'tanh',\n",
      "             'activity_regularizer': None,\n",
      "             'batch_input_shape': (None, 10, 7845),\n",
      "             'bias_constraint': None,\n",
      "             'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
      "             'bias_regularizer': None,\n",
      "             'dropout': 0.0,\n",
      "             'dtype': 'float32',\n",
      "             'go_backwards': False,\n",
      "             'implementation': 1,\n",
      "             'kernel_constraint': None,\n",
      "             'kernel_initializer': {'class_name': 'VarianceScaling',\n",
      "                                    'config': {'distribution': 'uniform',\n",
      "                                               'mode': 'fan_avg',\n",
      "                                               'scale': 1.0,\n",
      "                                               'seed': None}},\n",
      "             'kernel_regularizer': None,\n",
      "             'name': 'lstm_1',\n",
      "             'recurrent_activation': 'hard_sigmoid',\n",
      "             'recurrent_constraint': None,\n",
      "             'recurrent_dropout': 0.0,\n",
      "             'recurrent_initializer': {'class_name': 'Orthogonal',\n",
      "                                       'config': {'gain': 1.0, 'seed': None}},\n",
      "             'recurrent_regularizer': None,\n",
      "             'return_sequences': False,\n",
      "             'return_state': False,\n",
      "             'stateful': False,\n",
      "             'trainable': True,\n",
      "             'unit_forget_bias': True,\n",
      "             'units': 128,\n",
      "             'unroll': False,\n",
      "             'use_bias': True}},\n",
      " {'class_name': 'Dense',\n",
      "  'config': {'activation': 'linear',\n",
      "             'activity_regularizer': None,\n",
      "             'bias_constraint': None,\n",
      "             'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
      "             'bias_regularizer': None,\n",
      "             'kernel_constraint': None,\n",
      "             'kernel_initializer': {'class_name': 'VarianceScaling',\n",
      "                                    'config': {'distribution': 'uniform',\n",
      "                                               'mode': 'fan_avg',\n",
      "                                               'scale': 1.0,\n",
      "                                               'seed': None}},\n",
      "             'kernel_regularizer': None,\n",
      "             'name': 'dense_1',\n",
      "             'trainable': True,\n",
      "             'units': 7845,\n",
      "             'use_bias': True}},\n",
      " {'class_name': 'Activation',\n",
      "  'config': {'activation': 'softmax',\n",
      "             'name': 'activation_1',\n",
      "             'trainable': True}}]\n"
     ]
    }
   ],
   "source": [
    "pprint(model_simple.get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               4082688   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7845)              1012005   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 7845)              0         \n",
      "=================================================================\n",
      "Total params: 5,094,693\n",
      "Trainable params: 5,094,693\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_simple.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, everything looks fine here. Now, left visualize the training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2bea55278>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHPF/QL5HiMYTiB4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = history_dict['loss']\n",
    "validation_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = list(range(1, len(loss)+1))\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, validation_loss, 'r', label = 'Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's use this loaded model to make some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating from seed: \"to his cabin Even broken in spirit as he is\"\n",
      "to his cabin Even broken in spirit as he is => the \n",
      "his cabin Even broken in spirit as he is the => to \n",
      "cabin Even broken in spirit as he is the to => had \n",
      "Even broken in spirit as he is the to had => in \n",
      "broken in spirit as he is the to had in => the \n",
      "in spirit as he is the to had in the => your \n",
      "spirit as he is the to had in the your => of \n",
      "as he is the to had in the your of => of \n",
      "he is the to had in the your of of => life \n",
      "is the to had in the your of of life => in \n"
     ]
    }
   ],
   "source": [
    "test_idx = 5000 \n",
    "test_words = input_words[test_idx]\n",
    "print(\"Generating from seed: \\\"%s\\\"\" % (\" \".join(test_words)))\n",
    "\n",
    "for i in range(NUM_PREDS_PER_EPOCH):\n",
    "    Xtest = np.zeros((1, sequence_length, number_words))\n",
    "\n",
    "    for i, word in enumerate(test_words):\n",
    "        Xtest[0, i, word_dict[word]] = 1\n",
    "\n",
    "    pred = model_simple.predict(Xtest, verbose=0)[0]\n",
    "    ypred = word_list[np.argmax(pred)]\n",
    "    print(\" \".join(test_words), \"=>\", ypred, end=\" \")\n",
    "    \n",
    "    # Add the new word to the test instance and \n",
    "    # move the window forward 1 step\n",
    "    test_words = test_words[1:] + [ypred]\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
