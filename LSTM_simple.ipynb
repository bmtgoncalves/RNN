{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><h1>LSTM Tutorial in Keras</h1>\n",
    "<div>Bruno Gon√ßalves</div>\n",
    " <a href=\"http://www.bgoncalves.com/\" ><span style=\"font-size:90%;\">www.bgoncalves.com</span></a>\n",
    " </div>\n",
    "<!--h2 align=\"center\", style=\"font-size=150%\">Part I</h2-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Embedding\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7845\n",
      "['the', 'and', 'I', 'of', 'to', 'my', 'a', 'in', 'was', 'that', 'me', 'with', 'had', 'which', 'but', 'you', 'not', 'his', 'for', 'as']\n"
     ]
    }
   ],
   "source": [
    "filename = \"Frankenstein.txt\"\n",
    "\n",
    "word_regex = re.compile(r'\\w+', re.U)\n",
    "\n",
    "text = \"\".join(open(filename, 'rt').readlines())\n",
    "\n",
    "text_words = word_regex.findall(text)\n",
    "\n",
    "word_counts = Counter(text_words)\n",
    "\n",
    "word_count_items = list(word_counts.items())\n",
    "word_count_items.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "word_list = [word for word, count in word_count_items]\n",
    "word_dict = dict(zip(word_list, range(len(word_count_items))))\n",
    "\n",
    "number_words = len(word_counts)\n",
    "\n",
    "print(number_words)\n",
    "print(word_list[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create inputs and labels\n",
    "\n",
    "We use a window of size __sequence_length__ size that moves __step__ by step over the text. Each such sequence corresponds to a sentence and a row in __input_words__. The word at the next position is our expected __label_words__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78596\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 10\n",
    "step = 1\n",
    "\n",
    "input_words = []\n",
    "label_words = []\n",
    "for i in range(0, len(text_words) - sequence_length, step):\n",
    "    input_words.append(text_words[i:i + sequence_length])\n",
    "    label_words.append(text_words[i + sequence_length])\n",
    "    \n",
    "print(len(input_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to', 'his', 'cabin', 'Even', 'broken', 'in', 'spirit', 'as', 'he', 'is']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_words[5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we one hot encode our words, generating a 3D tensor **X** of dimensions *len(input_words)* $\\times$ *sequence_length* $\\times$ *number_words*. This tensor will be all zeros except for one position per vector which will be one. For the sake of memory efficiency, we define it as a boolean tensor.\n",
    "\n",
    "Conversely, our expected output **y** is now a 2D matrix of dimensions *len(input_words)* $\\times$ *number_words*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78596, 10, 7845)\n"
     ]
    }
   ],
   "source": [
    "X = np.zeros((len(input_words), sequence_length, number_words), dtype=np.bool)\n",
    "y = np.zeros((len(input_words), number_words), dtype=np.bool)\n",
    "\n",
    "for i, sentence in enumerate(input_words):\n",
    "    for t, word in enumerate(sentence):\n",
    "        X[i, t, word_dict[word]] = 1\n",
    "\n",
    "    y[i, word_dict[label_words[i]]] = 1\n",
    "    \n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some useful constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "NUM_ITERATIONS = 1\n",
    "NUM_EPOCHS_PER_ITERATION = 3\n",
    "NUM_PREDS_PER_EPOCH = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_simple = Sequential()\n",
    "model_simple.add(GRU(HIDDEN_SIZE, input_shape=(sequence_length, number_words)))\n",
    "model_simple.add(Dense(number_words))\n",
    "model_simple.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Compilation\n",
    "\n",
    "The final remaining step in defining our model is to \"compile\" it. This signals to Keras that we are done defining layers as well as defining the loss function and optimizer we wish to use. After this step we are ready to start learning our model!\n",
    "\n",
    "Here we opt to use the **RMSprop** optimizer with a learning rate of **0.01**, together with a **categorical_crossentropy** loss function. Naturally, other choices are possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=0.01)\n",
    "model_simple.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, let's take a look at the models internal representation. Keras provides us with a simple way of doing this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'class_name': 'LSTM',\n",
      "  'config': {'activation': 'tanh',\n",
      "             'activity_regularizer': None,\n",
      "             'batch_input_shape': (None, 10, 7845),\n",
      "             'bias_constraint': None,\n",
      "             'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
      "             'bias_regularizer': None,\n",
      "             'dropout': 0.0,\n",
      "             'dtype': 'float32',\n",
      "             'go_backwards': False,\n",
      "             'implementation': 1,\n",
      "             'kernel_constraint': None,\n",
      "             'kernel_initializer': {'class_name': 'VarianceScaling',\n",
      "                                    'config': {'distribution': 'uniform',\n",
      "                                               'mode': 'fan_avg',\n",
      "                                               'scale': 1.0,\n",
      "                                               'seed': None}},\n",
      "             'kernel_regularizer': None,\n",
      "             'name': 'lstm_1',\n",
      "             'recurrent_activation': 'hard_sigmoid',\n",
      "             'recurrent_constraint': None,\n",
      "             'recurrent_dropout': 0.0,\n",
      "             'recurrent_initializer': {'class_name': 'Orthogonal',\n",
      "                                       'config': {'gain': 1.0, 'seed': None}},\n",
      "             'recurrent_regularizer': None,\n",
      "             'return_sequences': False,\n",
      "             'return_state': False,\n",
      "             'stateful': False,\n",
      "             'trainable': True,\n",
      "             'unit_forget_bias': True,\n",
      "             'units': 128,\n",
      "             'unroll': False,\n",
      "             'use_bias': True}},\n",
      " {'class_name': 'Dense',\n",
      "  'config': {'activation': 'linear',\n",
      "             'activity_regularizer': None,\n",
      "             'bias_constraint': None,\n",
      "             'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
      "             'bias_regularizer': None,\n",
      "             'kernel_constraint': None,\n",
      "             'kernel_initializer': {'class_name': 'VarianceScaling',\n",
      "                                    'config': {'distribution': 'uniform',\n",
      "                                               'mode': 'fan_avg',\n",
      "                                               'scale': 1.0,\n",
      "                                               'seed': None}},\n",
      "             'kernel_regularizer': None,\n",
      "             'name': 'dense_1',\n",
      "             'trainable': True,\n",
      "             'units': 7845,\n",
      "             'use_bias': True}},\n",
      " {'class_name': 'Activation',\n",
      "  'config': {'activation': 'softmax',\n",
      "             'name': 'activation_1',\n",
      "             'trainable': True}}]\n"
     ]
    }
   ],
   "source": [
    "pprint(model_simple.get_config())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we three layers, **LSTM**, **Dense** and **Activation**. The values of all their parameters (mostly default) are also clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Traininig\n",
    "\n",
    "To train the model we need just call the model.fit method with the appropriate parameters. Here we define a training function that combines training multiple iterations with generating predictions based on a specific sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, val_split=0.1):\n",
    "    for iteration in range(NUM_ITERATIONS):\n",
    "        print(\"=\" * 50)\n",
    "        print(\"Iteration #: %d\" % (iteration))\n",
    "        \n",
    "        # We keep the history result of the trainig step. This provides us with \n",
    "        # valueable information about the training procedure that we may use later.\n",
    "        history = model.fit(X_train, y_train, \n",
    "                            validation_split=val_split,\n",
    "                            batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)\n",
    "\n",
    "        # Choose a specific sentence to use to generate the predictions \n",
    "        # and generate NUM_PREDS_PER_EPOCH predictions.\n",
    "        test_idx = 1502 \n",
    "        test_words = input_words[test_idx]\n",
    "        print(\"Generating from seed: \\\"%s\\\"\" % (\" \".join(test_words)))\n",
    "\n",
    "        for i in range(NUM_PREDS_PER_EPOCH):\n",
    "            Xtest = np.zeros((1, sequence_length, number_words))\n",
    "\n",
    "            for i, word in enumerate(test_words):\n",
    "                Xtest[0, i, word_dict[word]] = 1\n",
    "\n",
    "            pred = model.predict(Xtest, verbose=0)[0]\n",
    "            ypred = word_list[np.argmax(pred)]\n",
    "            print(\" \".join(test_words), \"=>\", ypred, end=\" \")\n",
    "\n",
    "            # Add the new word to the test instance and \n",
    "            # move the window forward 1 step\n",
    "            test_words = test_words[1:] + [ypred]\n",
    "\n",
    "            print()\n",
    "            \n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model!\n",
    "\n",
    "Unfortunately, this takes a looooooong time without a GPU, so let's use just a small subset of our data for a quick check that all is fine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Iteration #: 0\n",
      "Train on 1500 samples, validate on 500 samples\n",
      "Epoch 1/3\n",
      "1500/1500 [==============================] - 35s 23ms/step - loss: 8.2253 - val_loss: 7.4987\n",
      "Epoch 2/3\n",
      "1500/1500 [==============================] - 26s 17ms/step - loss: 6.4317 - val_loss: 7.9809\n",
      "Epoch 3/3\n",
      "1500/1500 [==============================] - 25s 17ms/step - loss: 6.2946 - val_loss: 8.2486\n",
      "Generating from seed: \"of success there will be none to participate my joy\"\n",
      "of success there will be none to participate my joy => a \n",
      "success there will be none to participate my joy a => a \n",
      "there will be none to participate my joy a a => a \n",
      "will be none to participate my joy a a a => a \n",
      "be none to participate my joy a a a a => a \n",
      "none to participate my joy a a a a a => a \n",
      "to participate my joy a a a a a a => a \n",
      "participate my joy a a a a a a a => a \n",
      "my joy a a a a a a a a => a \n",
      "joy a a a a a a a a a => a \n"
     ]
    }
   ],
   "source": [
    "history, model_simple = train_model(model_simple, X[:2000, :,:], y[:2000, :], 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model we trained for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides us with a simple way to save the model condiguration. Unfortunately, this does not include the training history, just the final result. We save history in a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_filename = \"lstm_bad\"\n",
    "history_dict = history.history\n",
    "\n",
    "model_simple.save(model_filename + \".h5\")\n",
    "json.dump(history_dict, open(model_filename + \".json\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load a well trained model that is available to us! We also load the training history so that we may visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history_dict = json.load(open(\"lstm_simple.json\", \"r\"))\n",
    "model_simple = load_model(\"lstm_simple.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the model architecture matches what we expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'class_name': 'LSTM',\n",
      "  'config': {'activation': 'tanh',\n",
      "             'activity_regularizer': None,\n",
      "             'batch_input_shape': (None, 10, 7845),\n",
      "             'bias_constraint': None,\n",
      "             'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
      "             'bias_regularizer': None,\n",
      "             'dropout': 0.0,\n",
      "             'dtype': 'float32',\n",
      "             'go_backwards': False,\n",
      "             'implementation': 1,\n",
      "             'kernel_constraint': None,\n",
      "             'kernel_initializer': {'class_name': 'VarianceScaling',\n",
      "                                    'config': {'distribution': 'uniform',\n",
      "                                               'mode': 'fan_avg',\n",
      "                                               'scale': 1.0,\n",
      "                                               'seed': None}},\n",
      "             'kernel_regularizer': None,\n",
      "             'name': 'lstm_1',\n",
      "             'recurrent_activation': 'hard_sigmoid',\n",
      "             'recurrent_constraint': None,\n",
      "             'recurrent_dropout': 0.0,\n",
      "             'recurrent_initializer': {'class_name': 'Orthogonal',\n",
      "                                       'config': {'gain': 1.0, 'seed': None}},\n",
      "             'recurrent_regularizer': None,\n",
      "             'return_sequences': False,\n",
      "             'return_state': False,\n",
      "             'stateful': False,\n",
      "             'trainable': True,\n",
      "             'unit_forget_bias': True,\n",
      "             'units': 128,\n",
      "             'unroll': False,\n",
      "             'use_bias': True}},\n",
      " {'class_name': 'Dense',\n",
      "  'config': {'activation': 'linear',\n",
      "             'activity_regularizer': None,\n",
      "             'bias_constraint': None,\n",
      "             'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
      "             'bias_regularizer': None,\n",
      "             'kernel_constraint': None,\n",
      "             'kernel_initializer': {'class_name': 'VarianceScaling',\n",
      "                                    'config': {'distribution': 'uniform',\n",
      "                                               'mode': 'fan_avg',\n",
      "                                               'scale': 1.0,\n",
      "                                               'seed': None}},\n",
      "             'kernel_regularizer': None,\n",
      "             'name': 'dense_1',\n",
      "             'trainable': True,\n",
      "             'units': 7845,\n",
      "             'use_bias': True}},\n",
      " {'class_name': 'Activation',\n",
      "  'config': {'activation': 'softmax',\n",
      "             'name': 'activation_1',\n",
      "             'trainable': True}}]\n"
     ]
    }
   ],
   "source": [
    "pprint(model_simple.get_config())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, everything looks fine here. Now, left visualize the training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2c09993c8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XucVVX9//HXBxgEBgRFTJHLkKRy\nv42YISBS3tAIpYLHYEoqhJVmavJTCysty/utFL9fLRVQI9BSFEhRMPspw/2meQEJIRgw7hCX+Xz/\nWGeG2zCcM3P2nNv7+Xicxzlnzz57r+2Wz157rbU/y9wdERHJfrVSXQAREakZCvgiIjlCAV9EJEco\n4IuI5AgFfBGRHKGALyKSIxTwRURyhAK+iEiOUMAXEckRdVJdgP0dd9xxXlBQkOpiiIhkjDlz5qx3\n92bxrJtWAb+goIDi4uJUF0NEJGOY2afxrqsmHRGRHKGALyKSIxTwRURyhAK+iEiOiDTgm9l1ZrbY\nzJaY2Y+i3JeIiFQusoBvZh2Bq4GeQBfgIjNrG9X+RESkclHW8NsB77r7dnffA7wFXBLh/kREpBJR\nBvzFQG8za2pmDYALgZYHr2RmI8ys2MyKS0pKIiyOiGS8nTvh97+Hf/4z1SXJSJEFfHdfBvwGmAa8\nBswH9law3lh3L3T3wmbN4npYTERy0YIFcPrpcM010Lkz/OY3sGdPqkuVUSLttHX3/3X3Hu7eB/gP\noMuyiCRm7164+27o2RPWr4fx4+Gii2D0aDjjjHAhSEf/+Q9s357qUhwg0tQKZna8u68zs1aE9vsv\nR7k/EckyK1bA5ZfDzJlwySXw+ONw3HEwdCj8+c/w/e9DYWEI/rfdBkcdlbqybtsGb70F06eH15Il\nYfnRR8OJJ0Lz5uH9cK/GjSMvorl7dBs3mwU0BXYDP3b31ytbv7Cw0JVLR+QISkvh7behSRPo0AFq\n1051iZLPHZ55Bn7wg/D94YfhO98BswPX+/xzuP56ePppaNcOnnwSvlxD9cq9e2HOnH0B/p13YPfu\ncNHp3RvOOSest2bNvtfq1eF9584Dt3XssbBhQ5WKYWZz3L0wnnUjreG7e+8oty+SVDt3wh//GP6h\nfulLqS7Nofbsgeefh1//el/tsWHDUMM944wQ6M44I9QWo1JaCpMmhU7ToiJo3Tr5+1i/Hr73vVCD\n7907BPPDZdE99thwzoYMgZEj4StfgeuugzvugPz85Jdt1Sp45ZUQ4N94IzTbAHTtCj/6EZx7LvTq\nBfXrH34b7rBp04EXgv/+N/llrXjfnjavHj16uEhKvPqq+8knu4N748bur72W6hLt89//uj/xxL7y\ndejg/sc/uj/zjPsPfuB++unueXnhb+DesqX7N7/pfs897rNmuW/fXv0ylJa6T5rk3qnTvv3UquU+\ncKD7tGnh78nw6qvuJ5wQjuc3v3Hfsyf+327e7H7NNaFsX/yi++uvJ6dMK1a433uv+5ln7jv2Fi3c\nhw93Hz/efe3a5OynioBijzPGpjzI7/9SwJca969/uQ8eHP4pnHJK+AfcpUsIZvfdl7xAVhXbtrk/\n+GAILuBeWOg+ebL73r2Hrrtjh/s777jff7/7kCHuBQX7glODBu7DhrlPnZpYAHUPx//yy+7du4dt\nfelL7s8+6758ufstt7g3axaWn3pqKOvGjVU71rVr9wXrjh3d58+v2nbc3d96y71t27CtSy4JF76p\nU90/+yz+8/nxx+GCc/rp+/47duvmfued7kuXpvb/i4Mo4Iscya5dIRDk57vXq+d+xx3uO3eGv23Z\n4j5oUPjnMXz4vuU1ZdMm97vucj/++FCG3r3DHUeiQebf/3Z/6SX3kSPdmzQJ2zrxRPcbb3RfsKDy\n35aWhpr7GWeE37Vp4/7UU+67dx+43s6d4U6jbL38fPdRo9wXLz78dlesCHcLP/2p+0UXuZ90Uvit\nmfsNN4SLV3Vt3+5+883heMsCNrgfe6x7377hzujxx8NFctOm8JsPPnD/1a9CYC9bv7AwBP6PPqp+\nmSKigC9SmVmzQi0S3AcMcP/kk0PX2bvX/Wc/C+v06hWCZ9Q2bHAfM8b9mGPCfs89N9RWk2HHDveJ\nE0MTTJ06YftduoSL3urVB6775pvhIlPWPPT44+ECeSSzZ7tfcYX7UUeF3559tvuECeGO4IYb3M85\nZ9+xlTUJtW/vXlQUyjF3bnKO9WAlJe4zZrg/9JD7iBGhaaZRowMvBGUXV3D/8pdDeZYvj6Y8SaaA\nL1KRdetCQAL3Vq3cX3zxyLXm5593r18/BL5586Ip1+rVodadnx/K9o1vuL/3XjT7cg8B8JFH3Hv2\n3Bd4zzvP/Xe/c+/ff9+dwCOPVO3upqQk3KG0br0viNatG2rLV18d9vOPf4Qmq1QpLQ0B/a9/DbX6\nyy8PzWErV6auTFWkgC+yv7173R97LNQu69RxHz3afevW+H9fXBza0Rs0CLXkZPn4Y/fvfS8Ew1q1\nQk130aLkbT8e77/vfuut+4Lz8ceHwJeMjt49e8LdwoIF8d0hSJUo4IuUmTFjX4fj2We7L1lSte2s\nWRNu9cH95z+vXqfd4sWhE7V27RDsR45MfRvx3r0hMCdyIZS0kEjA1wQokp0+/BAGDYJ+/aCkBMaN\nC+Om27ev2vZOOAFmzAgP/4wZA9/+dniyMhGzZ4cydewIkyeHcdvLl8Njj8HJJ1etXMlSq1bITxPF\n2HVJG5E+eCVS4z7/HH75S3jkEahXD+68MzyJWdmDMPGqVw/+8Afo1Al+8pPwtGvLluGR+KOPPvyr\nVi144gn429/gmGPCBeOHP4SmTatfJpEEKOBL9BYvDk9B5ufDxReH1+GenKyqXbvgd7+DX/wiPMV4\n5ZXh8wknJHc/ZnDjjSHoP/lk2NfmzeGR+bLPW7aErsr9nXBCSAA2ciQ0apTcMonEKdJcOolSLp0s\nNGlSaAZp2DDkfvngg7C8Y8eQ8fDii0M6gKrmg3GHl16Cm26Cjz6Cr34V7r03NE+kSmlpaO7ZvDm8\ntm0Lx1uvXurKJFkrkVw6asOXaJSWhqaLSy8NwW7uXHj//ZCD5d57oVmzUOPt1SvUfq+4AiZODAGy\nIu6wY0dIMLVyJSxbFtrk+/UL7eJ5eSHHybRpqQ32EJpwGjWCk04KCb0KCxXsJS2ohi/Jt3kzXHYZ\n/OUvMHx4aGqpKOBt3AivvQYvvwxTpoREVHl54QKxe3eoGW/bFnKKb98eLiIHO+640HRz9dVQRy2U\nknvSJlum5KAPP4SBA0NN/qGHQnrbg1PalmnSJGQ5HDIkZIJ8550Q/BctCp2s+fnh1aBBxZ/z86FP\nnxrJIy6SDRTwJXmmTg3Bu3bt0LRSlg88HnXqhODdp0905RPJcWrDl+pzD+3xF14IrVqF8eaJBHsR\nqRGq4Uv17NgBV10V5hn95jfhqaf08I5ImlINX6pu1So46yyYMCE84PT88wr2ImlMNXypmtWr4eyz\nYd26MBrnootSXSIROQIFfEnc2rXQv394nz695iaNFpFqUcCXxKxfH55mXbkyjKFXsBfJGAr4Er+N\nG+Hcc8NY+1degd69U10iEUmAAr7EZ/NmOP98WLIEXnwxNOmISEZRwJcj27oVBgyAOXNCvpsLLkh1\niUSkChTwpXI7dsDXvx7SHjz3XEibICIZSQFfDu+//w2ZKN98E55+OjxYJSIZK9IHr8zsejNbYmaL\nzWyCmSlHbKbYtSsE+KlTw2xNw4alukQiUk2RBXwzOwm4Fih0945AbWBIVPuTJNqzB4qK4K9/hUcf\nDbNHiUjGi7pJpw5Q38x2Aw2A1RHvTypTUhKm4du6NUzDt3XrgZ/L3ufMCQ9U3XcfXHNNqkstIkkS\nWcB398/M7B5gJbADmObu06Lan1TCHUaNgscfP/K6deuG/PL33hsm/xaRrBFZwDezY4CBQBtgI/An\nMxvm7s8etN4IYARAq1atoipO7nKHH/84BPuRI8OUgg0bhin4KnqvWzfVJRaRiETZpPNVYLm7lwCY\n2STgK8ABAd/dxwJjIUxxGGF5ctPtt8MDD8C114b3w80+JSJZL8pROiuBL5tZAzMzoD+wLML9ycHu\nuSfM9/rd78L99yvYi+S4yAK+u78LTATmAoti+xob1f7kII8/DjfdBN/6FowdC7U09YFIrot0lI67\njwHGRLkPqcC4caGTdsAAeOaZMMesiOQ8VfuyzUsvweWXQ9++8Kc/qRNWRMop4GeT6dNDE05hYZiF\nqn79VJdIRNKIAn62+Pvf4RvfgNNOgylTwjBLEZH9KOBng7lz4cILoUULmDYNjj021SUSkTSkgJ/p\nli6F886DJk3gb3+DL3wh1SUSkTSlgJ+p9uwJQy/79oU6deD116Fly1SXSkTSmAJ+pnGHl1+Gzp3h\ne9+DU0+FN96Atm1TXTIRSXMK+Jlk7twwl+zFF4ca/qRJMGsWtGuX6pKJSAZQwM8EK1fCZZdBjx6w\naBE8/HCYTHzQIKVLEJG4aYrDdLZpE/z61/uSno0eHV6NG6e6ZCKSgRTw09Hu3fDYY/Dzn8Pnn4fa\n/R13qFNWRKpFAT/dzJkTslsuXAjnnBMyXnbrlupSiUgWUBt+uti5E265Bc44I0xFOHlyGFevYC8i\nSaIafjr4xz9Crf7992H48DC94DHHpLpUIpJlVMNPpe3bw/SDvXqFz6+9Bk8+qWAvIpFQDT9V3nwT\nrroKPv4YrrkG7rpLCc9EJFKq4de0LVtCgO/XL3x/80149FEFexGJnAJ+TZo2DTp2DEMuf/zjMBKn\nb99Ul0pEcoQCfk3YujXkvTnvPGjQIOSuv/fe8FlEpIYo4Eft7behS5cwkfiNN8K8eXDmmakulYjk\nIAX8qOzcCT/5CfTpEzJcvvUW3H031KuX6pKJSI7SKJ0ozJsX0iEsWQIjRoSnZdUpKyIpphp+Mu3Z\nA7/8JfTsGXLgTJkSJilRsBeRNKAafrK8/z585zswezYMHQqPPKK5ZUUkraiGX12lpfDggyHnzccf\nw/PPw/jxCvYiknZUw6+O0lK48kr4wx9gwAB44gk48cRUl0pEpEKq4VeVO/zoRyHYjxkDf/2rgr2I\npLXIAr6ZnWpm8/d7bTazH0W1vxr3s5+FqQZvuCEEfE01KCJpLrImHXf/AOgKYGa1gc+AyVHtr0bd\nc0+Ygeqqq8LYegV7EckANdWk0x/42N0/raH9RWfsWLjpJvj2t0NOHAV7EckQNRXwhwATamhf0Zkw\nIeTEufBCePppqF071SUSEYlb5AHfzOoCXwf+dJi/jzCzYjMrLikpibo4Vffyy2GcfZ8+MHEi1K2b\n6hKJiCSkJmr4FwBz3X1tRX9097HuXujuhc2aNauB4lTBjBkweHAYa/+Xv0D9+qkukYhIwmoi4A8l\nk5tz3n0Xvv51aNsWXn0Vjj461SUSEamSSAO+meUDXwMmRbWPceOgoABq1Qrv48YlceOLFsEFF8Dx\nx8P06dC0aRI3LiJSsyJ90tbdtwGRRclx40Iyyu3bw/dPPw3fAYqKqrnxjz6Cr30tTFLyt7/poSoR\nyXgZ/aTtrbfuC/Zltm8Py6vl44+hf3/YuzfU7Nu0qeYGRURSL6MD/sqViS2Py9Kl0Ls3bNsW5qBt\n164aGxMRSR8ZHfBbtUps+RHNmxcmFS+boapbtyqXTUQk3WR0wL/zzkPnAW/QICxP2D/+Af36hQ3M\nmgUdOiSljCIi6SKjA35RUch00Lp1yHDQunX4nnCH7Ztvhg7aZs1g5swwBFNEJMtkfD78oqJqjsh5\n9VW45BL44hc1GkdEslpG1/CrbdIkGDgwdMy+9ZaCvYhktdwN+M8+C9/6FhQWwhtvwHHHpbpEIiKR\nys2AP3bsvkRo06ZBkyapLpGISORyL+Dffz+MHBlSJrzyCjRsmOoSiYjUiIzvtAXCk7Dbt4fx84d7\nlZaG961b4dJLYfx4pTgWkZySHQH/4oth164wNvNIr5Yt4dproU52HLqISLyyI+o99FCqSyAikvZy\nrw1fRCRHxRXwzexkMzsq9vlsM7vWzDS0RUQkg8Rbw/8zsNfM2gJjgZbA+MhKJSIiSRdvwC919z3A\nIOBhd78J0GOpIiIZJN6Av9vMhgKXAy/HluVFUyQREYlCvAF/OHAmcKe7LzezNsAz0RVLRESSLa5h\nme6+FLgWwMyOARq5+2+iLJiIiCRXvKN03jSzo83sWGAu8ISZ3Rdt0UREJJnibdJp7O6bgUuAp939\nDOCr0RVLRESSLd6AX8fMTgS+xb5OWxERySDxBvxfAFOBj919tpl9EfgwumKJiEiyxdtp+yfgT/t9\n/wS4NKpCiYhI8sXbadvCzCab2brY689m1iLqwomISPLE26TzFPAXoHns9dfYMhERyRDxBvxm7v6U\nu++Jvf4ANDvSj8ysiZlNNLP3zWyZmZ1ZrdKKiEiVxRvwN5jZMDOrHXsNAzbE8bsHgdfc/TSgC7Cs\nqgUVEZHqiTfgf5cwJPPfwBpgMHBFZT8ws8ZAH+B/Adx9l7tvrHJJq2ncOCgogFq1wvu4cakqiYhI\nasQV8N39U3f/urs3c/fj3f0bHHmUThugBHjKzOaZ2f+YWf7BK5nZCDMrNrPikpKSxI8gDuPGwYgR\n8OmnYVrbTz8N3xX0RSSXmLtX7YdmK929VSV/LwT+P9DL3d81sweBze7+08P9prCw0IuLi6tUnsoU\nFIQgf7DWrWHFiqTvTkSkxpjZHHcvjGfd6kxxaEf4+ypglbu/G/s+Eehejf1V2cqViS0XEclG1Qn4\nld4auPu/gX+Z2amxRf2BpdXYX5W1Osx9yOGWi4hko0oDvpltMbPNFby2EMbjH8kPgXFmthDoCvwq\nCWVO2J13QoMGBy5r0CAsFxHJFZWmVnD3RtXZuLvPB+JqW4pSUVF4v/XW0IzTqlUI9mXLRURyQXWa\ndDJKUVHooC0tDe+VBXsN4RSRbBRX8rRcUjaEc/v28L1sCCfojkBEMlvO1PDjdeut+4J9me3bw3IR\nkUymgH8QDeEUkWylgH+QRIZwqq1fRDKJAv5B4h3CqXQNIpJpFPAPUlQEY8eGtAtm4X3s2EM7bNXW\nLyKZpsq5dKIQVS6dKNSqFWr2BzMLQz9FRGpCTeXSyWlK1yAimUYBv4qUrkFEMo0CfhXF29YvIpIu\n9KRtNRQVKcCLSOZQDV9EJEco4NcQPaQlIqmmJp0aoIRsIpIOVMOvAXpIS0TSgQJ+DVBCNhFJBwr4\nNUAPaYlIOlDArwGJPKSlzl0RiYoCfg2I9yEtZeAUkSgpeVoaKSgIQf5grVuHeXhFRA6m5GkZSp27\nIhIlBfw0os5dEYmSAn4aUQZOEYmSAn4aUQZOEYmSAn6aKSoKHbSlpeG9smCvIZwikgjl0slQys8j\nIomKtIZvZivMbJGZzTez3B1vGQHl5xGRRNVEDb+fu6+vgf3kFA3hFJFEqQ0/Q2kIp4gkKuqA78A0\nM5tjZiMqWsHMRphZsZkVl5SURFyc7KEhnCKSqKgD/lnu3h24APi+mfU5eAV3H+vuhe5e2KxZs4iL\nkz00hFNEEhVpG767fxZ7X2dmk4GewMwo95lLNIm6iCQishq+meWbWaOyz8C5wOKo9ieHp/H6IgLR\n1vC/AEw2s7L9jHf31yLcn1RA4/VFpIzSI2c5pVwWyW5KjyzlNF5fRMoo4Gc5jdcXkTIK+FlO4/VF\npIwCfpbTeH0RKaOAnwOUcllEQOmRZT8awimS3VTDl3JKuSyS3RTwpZyGcIpkNwV8KachnCLZTQFf\nymkIp0h2U8CXcokM4dRoHpHMo1E6coB4Ui5rNI9IZlINXxKm0TwimUkBXxKm0TwimUkBXxKm0Twi\nmUkBXxKW6GgedfCKpAcFfElYoqN5RowIHbvu+zp4FfRFap5mvJJIacYtkWhpxitJG+rgFUkfCvgS\nKXXwiqQPBXyJVCIdvOrcFYmWAr5EKt4OXnXuikQv7Tttd+/ezapVq9i5c2eKSiVVUa9ePVq0aEFe\nXl5c66tzV6RqEum0TftcOqtWraJRo0YUFBRgZqkujsTB3dmwYQOrVq2iTZs2cf0m0c7dceNCKoeV\nK0N/wJ13Ko+PyJGkfZPOzp07adq0qYJ9BjEzmjZtmtBdWSKdu2r+EamatA/4gIJ9Bkr0nCXSuavk\nbSJVE3nAN7PaZjbPzF6Oel/JtmHDBrp27UrXrl054YQTOOmkk8q/79q1K65tDB8+nA8++KDSdR59\n9FHGJal6etZZZzF//vykbKsmJfL0biLNPxr5I7JPTbThXwcsA46ugX0ltW23adOm5cHz9ttvp2HD\nhtx4440HrOPuuDu1alV87XzqqaeOuJ/vf//7VStgloknFz+E81pRB+/BzT/K2y9yoEhr+GbWAhgA\n/E+U+ylTU227H330Ee3bt6eoqIgOHTqwZs0aRowYQWFhIR06dOAXv/hF+bplNe49e/bQpEkTRo8e\nTZcuXTjzzDNZt24dALfddhsPPPBA+fqjR4+mZ8+enHrqqbzzzjsAbNu2jUsvvZT27dszePBgCgsL\n467J79ixg8svv5xOnTrRvXt3Zs6cCcCiRYs4/fTT6dq1K507d+aTTz5hy5YtXHDBBXTp0oWOHTsy\nceLEZP6nS4p4m3/U9CNyoKibdB4AfgKURrwfoGb/gb///vtcf/31LF26lJNOOom77rqL4uJiFixY\nwPTp01m6dOkhv9m0aRN9+/ZlwYIFnHnmmTz55JMVbtvdee+997j77rvLLx4PP/wwJ5xwAkuXLuWn\nP/0p8+bNi7usDz30EEcddRSLFi3imWee4bLLLmPXrl387ne/48Ybb2T+/PnMnj2b5s2bM2XKFAoK\nCliwYAGLFy/ma1/7WtX+A0Uo3uafqoz8UfOPZLPIAr6ZXQSsc/c5R1hvhJkVm1lxSUlJtfZZk3lb\nTj75ZAoL9w19nTBhAt27d6d79+4sW7aswoBfv359LrjgAgB69OjBisMMML/kkksOWeftt99myJAh\nAHTp0oUOHTrEXda3336bYcOGAdChQweaN2/ORx99xFe+8hXuuOMOfvvb3/Kvf/2LevXq0blzZ157\n7TVGjx7N3//+dxo3bhz3fmpSUVEYn19aGt4raqLRyB+RA0VZw+8FfN3MVgDPAeeY2bMHr+TuY929\n0N0LmzVrVq0d1mTelvz8/PLPH374IQ8++CBvvPEGCxcu5Pzzz69wSGLdunXLP9euXZs9e/ZUuO2j\njjrqiOskw2WXXcbkyZM56qijOP/885k5cybt2rWjuLiYDh06MHr0aH71q19Ftv+oRTXyR3cCkqki\nC/ju/v/cvYW7FwBDgDfcfVhU+4PEJ+ZIls2bN9OoUSOOPvpo1qxZw9SpU5O+j169evHCCy8Aoe29\nojuIw+ndu3f5KKBly5axZs0a2rZtyyeffELbtm257rrruOiii1i4cCGfffYZDRs25LLLLuOGG25g\n7ty5ST+WmhLFyB/dCUgmS/snbRNR9g+5pp/A7N69O+3bt+e0006jdevW9OrVK+n7+OEPf8h3vvMd\n2rdvX/46XHPLeeedV57SoHfv3jz55JOMHDmSTp06kZeXx9NPP03dunUZP348EyZMIC8vj+bNm3P7\n7bfzzjvvMHr0aGrVqkXdunV57LHHkn4sNSnZI38quxM43AQweiJY0kbZsMJ0ePXo0cMPtnTp0kOW\n5aLdu3f7jh073N39n//8pxcUFPju3btTXKrKZdK5e/ZZ9wYN3EO9PbwaNAjL92d24DplL7Oqb7Ns\n3datw3Zat654HZGKAMUeZ4zNqhp+Ntu6dSv9+/dnz549uDuPP/44dero9CVLvHeH8d4JlG0rnrsB\nPS8gNSUjUisINGnShDlz5rBgwQIWLlzIueeem+oiZZ14Rv4k0k8Ub79AosOJ1WksVaWAL5KARDqC\n4x01lmiqCHUaS1Up4IskKJ47AYj/biCR4cQaPirVoYAvEpF47waiaCZK9E5AF4fcoIAvEqF47gai\naCZK9E4g3ouDLgyZTQH/CPr163fIg1QPPPAAo0aNqvR3DRs2BGD16tUMHjy4wnXOPvtsDp7S8WAP\nPPAA2/f7l3vhhReycePGeIpeqdtvv5177rmn2tuR5Eh2M1Ei/QLxXhx015D5FPCPYOjQoTz33HMH\nLHvuuecYOnRoXL9v3rx5tTJOHhzwp0yZQpMmTaq8Pcls8d4NJNIvEMVoIt01pCcF/CMYPHgwr7zy\nSvmEJytWrGD16tX07t27fGx89+7d6dSpEy+99NIhv1+xYgUdO3YEQpriIUOG0K5dOwYNGsSOHTvK\n1xs1alR5euUxY8YAIcvl6tWr6devH/369QOgoKCA9evXA3DffffRsWNHOnbsWJ5eecWKFbRr146r\nr76aDh06cO655x6wnyOpaJvbtm1jwIAB5SmTn3/+eQBGjx5N+/bt6dy58yHzBEh0kj18NIrRRLpr\nSFPxPqFVE68jPml73XXuffsm93XddUd8km3AgAH+4osvurv7r3/9a7/hhhvcPTz9umnTJnd3Lykp\n8ZNPPtlLS0vd3T0/P9/d3ZcvX+4dOnRwd/d7773Xhw8f7u7uCxYs8Nq1a/vs2bPd3X3Dhg3u7r5n\nzx7v27evL1iwwN3dW7du7SUlJeVlKfteXFzsHTt29K1bt/qWLVu8ffv2PnfuXF++fLnXrl3b582b\n5+7u3/zmN/2ZZ5455JjGjBnjd9999wHLDrfNiRMn+lVXXVW+3saNG339+vV+yimnlB/vf/7zn0P2\nkUlP2majeJ/ejfeJ4NatK37KuHXrQ7cZ7xPJiWxTTy5XjASetFUNPw77N+vs35zj7txyyy107tyZ\nr371q3z22WesXbv2sNuZOXNmeZrizp0707lz5/K/vfDCC3Tv3p1u3bqxZMmSIyZHe/vttxk0aBD5\n+fk0bNiQSy65hFmzZgHQpk0bunbtClSehjnebXbq1Inp06dz8803M2vWLBo3bkzjxo2pV68eV155\nJZMmTaLBwdVJSbl4+wWiGE2UjXcN2XB3kVnP5seaGGrawIEDuf7665k7dy7bt2+nR48eAIwbN46S\nkhLmzJlDXl4eBQUFFaZFPpLly5dzzz33MHv2bI455hiuuOKKKm2nTFl6ZQgplhNp0qnIKaecwty5\nc5kyZQq33XYb/fv352c/+xlorJVAAAAH7klEQVTvvfcer7/+OhMnTuSRRx7hjTfeqNZ+JHXiSTKX\nSHLCO+88MF0EHP4ZhHhTVSSjr+Hgssab1iLR9BfpmjRPNfw4NGzYkH79+vHd7373gM7aTZs2cfzx\nx5OXl8eMGTP4tKL/c/fTp08fxo8fD8DixYtZuHAhENIr5+fn07hxY9auXcurr75a/ptGjRqxZcuW\nQ7bVu3dvXnzxRbZv3862bduYPHkyvXv3rtZxHm6bq1evpkGDBgwbNoybbrqJuXPnsnXrVjZt2sSF\nF17I/fffz4IFC6q1b8kMuXrXkC0d1plVw0+hoUOHMmjQoANG7BQVFXHxxRfTqVMnCgsLOe200yrd\nxqhRoxg+fDjt2rWjXbt25XcKXbp0oVu3bpx22mm0bNnygPTKI0aM4Pzzz6d58+bMmDGjfHn37t25\n4oor6NmzJwBXXXUV3bp1i7v5BuCOO+4o75gFWLVqVYXbnDp1KjfddBO1atUiLy+P3//+92zZsoWB\nAweyc+dO3J377rsv7v1Kbsimu4ZkXURSnjQv3sb+mngpPXJ20bmTZIunMzaRzt14O41T3WFdGdRp\nKyLZKNlPLsfbpJTqpqdkUcAXkayT7L6GKC4iNTkHdxkFfBHJaYlcHFLVYZ0sGdFp6+6YWaqLIQkI\nTYsiuSnZHdbJkvYBv169emzYsIGmTZsq6GcId2fDhg3Uq1cv1UURSWvxXBiSKe0DfosWLVi1ahUl\nJSWpLookoF69erRo0SLVxRCR/aR9wM/Ly6NNmzapLoaISMZTp62ISI5QwBcRyREK+CIiOcLSafic\nmZUA+2fBOA5Yn6LiRCXbjinbjgey75iy7Xgg+46pOsfT2t2bxbNiWgX8g5lZsbsXprocyZRtx5Rt\nxwPZd0zZdjyQfcdUU8ejJh0RkRyhgC8ikiPSPeCPTXUBIpBtx5RtxwPZd0zZdjyQfcdUI8eT1m34\nIiKSPOlewxcRkSRJ24BvZueb2Qdm9pGZjU51earLzFaY2SIzm29mxakuT1WY2ZNmts7MFu+37Fgz\nm25mH8bej0llGRNxmOO53cw+i52n+WZ2YSrLmCgza2lmM8xsqZktMbPrYssz8jxVcjwZe57MrJ6Z\nvWdmC2LH9PPY8jZm9m4s5j1vZnWTvu90bNIxs9rAP4GvAauA2cBQd1+a0oJVg5mtAArdPWPHDptZ\nH2Ar8LS7d4wt+y3wubvfFbswH+PuN6eynPE6zPHcDmx193tSWbaqMrMTgRPdfa6ZNQLmAN8AriAD\nz1Mlx/MtMvQ8WUj7m+/uW80sD3gbuA74MTDJ3Z8zs8eABe7++2TuO11r+D2Bj9z9E3ffBTwHDExx\nmXKeu88EPj9o8UDgj7HPfyT8Y8wIhzmejObua9x9buzzFmAZcBIZep4qOZ6MFZuKdmvsa17s5cA5\nwMTY8kjOUboG/JOAf+33fRUZfpIJJ3Samc0xsxGpLkwSfcHd18Q+/xv4QioLkyQ/MLOFsSafjGj6\nqIiZFQDdgHfJgvN00PFABp8nM6ttZvOBdcB04GNgo7vvia0SScxL14Cfjc5y9+7ABcD3Y80JWcVD\n+2D6tREm5vfAyUBXYA1wb2qLUzVm1hD4M/Ajd9+8/98y8TxVcDwZfZ7cfa+7dwVaEFo0TquJ/aZr\nwP8MaLnf9xaxZRnL3T+Lva8DJhNOcjZYG2tnLWtvXZfi8lSLu6+N/WMsBZ4gA89TrF34z8A4d58U\nW5yx56mi48mG8wTg7huBGcCZQBMzK5ujJJKYl64BfzbwpVivdV1gCPCXFJepyswsP9bhhJnlA+cC\niyv/Vcb4C3B57PPlwEspLEu1lQXFmEFk2HmKdQj+L7DM3e/b708ZeZ4OdzyZfJ7MrJmZNYl9rk8Y\nnLKMEPgHx1aL5Byl5SgdgNgwqweA2sCT7h7hXO7RMrMvEmr1EGYZG5+Jx2NmE4CzCZn91gJjgBeB\nF4BWhEyn33L3jOgIPczxnE1oJnBgBTByv7bvtGdmZwGzgEVAaWzxLYR274w7T5Ucz1Ay9DyZWWdC\np2xtQqX7BXf/RSxOPAccC8wDhrn7f5O673QN+CIiklzp2qQjIiJJpoAvIpIjFPBFRHKEAr6ISI5Q\nwBcRyREK+JL1zGzvflkV5ycz+6qZFeyfbVMkndU58ioiGW9H7DF2kZymGr7krNgcBb+NzVPwnpm1\njS0vMLM3Yom5XjezVrHlXzCzybE85gvM7CuxTdU2sydiuc2nxZ6exMyujeVxX2hmz6XoMEXKKeBL\nLqh/UJPOt/f72yZ37wQ8QniyG+Bh4I/u3hkYBzwUW/4Q8Ja7dwG6A0tiy78EPOruHYCNwKWx5aOB\nbrHtfC+qgxOJl560laxnZlvdvWEFy1cA57j7J7EEXf9296Zmtp4w6cbu2PI17n6cmZUALfZ/3D2W\nsne6u38p9v1mIM/d7zCz1wgTrLwIvLhfDnSRlFANX3KdH+ZzIvbPd7KXfX1jA4BHCXcDs/fLhCiS\nEgr4kuu+vd/7P2Kf3yFkaAUoIiTvAngdGAXlE1g0PtxGzawW0NLdZwA3A42BQ+4yRGqSahySC+rH\nZhcq85q7lw3NPMbMFhJq6UNjy34IPGVmNwElwPDY8uuAsWZ2JaEmP4ow+UZFagPPxi4KBjwUy30u\nkjJqw5eclQ0Ty4skQk06IiI5QjV8EZEcoRq+iEiOUMAXEckRCvgiIjlCAV9EJEco4IuI5AgFfBGR\nHPF/QL5HiMYTiB4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = history_dict['loss']\n",
    "validation_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = list(range(1, len(loss)+1))\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, validation_loss, 'r', label = 'Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's use this loaded model to make some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating from seed: \"to his cabin Even broken in spirit as he is\"\n",
      "to his cabin Even broken in spirit as he is => or \n",
      "his cabin Even broken in spirit as he is or => to \n",
      "cabin Even broken in spirit as he is or to => be \n",
      "Even broken in spirit as he is or to be => his \n",
      "broken in spirit as he is or to be his => through \n",
      "in spirit as he is or to be his through => as \n",
      "spirit as he is or to be his through as => the \n",
      "as he is or to be his through as the => few \n",
      "he is or to be his through as the few => it \n",
      "is or to be his through as the few it => is \n"
     ]
    }
   ],
   "source": [
    "test_idx = 5000 \n",
    "test_words = input_words[test_idx]\n",
    "print(\"Generating from seed: \\\"%s\\\"\" % (\" \".join(test_words)))\n",
    "\n",
    "for i in range(NUM_PREDS_PER_EPOCH):\n",
    "    Xtest = np.zeros((1, sequence_length, number_words))\n",
    "\n",
    "    for i, word in enumerate(test_words):\n",
    "        Xtest[0, i, word_dict[word]] = 1\n",
    "\n",
    "    pred = model_simple.predict(Xtest, verbose=0)[0]\n",
    "    ypred = word_list[np.argmax(pred)]\n",
    "    print(\" \".join(test_words), \"=>\", ypred, end=\" \")\n",
    "    \n",
    "    # Add the new word to the test instance and \n",
    "    # move the window forward 1 step\n",
    "    test_words = test_words[1:] + [ypred]\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
